# src/crb/pipelines/collect_activations.py
from __future__ import annotations
import os
import re
import json
import argparse
import hashlib
import numpy as np
import torch
from typing import List, Dict, Any
import tempfile
import shutil

from transformers import PreTrainedTokenizer
from crb.models.qwen_loader import load_qwen, QwenLoadConfig
from crb.data.gsm8k import iter_gsm8k, extract_final_number, equals_num
from crb.instrumentation.layer_selectors import pick_blocks_by_indices
from crb.instrumentation.activation_recorder import ActivationRecorder
from crb.instrumentation.reasoning_mask import compute_reasoning_mask

def build_messages(question: str, system: str, user_suffix: str) -> List[Dict[str, str]]:
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": question.strip() + (user_suffix or "")},
    ]

def encode_chat(tokenizer: PreTrainedTokenizer, messages: List[Dict[str,str]]) -> torch.Tensor:
    """ä½¿ç”¨ Qwen/chat æ¨¡æ¿ç¼–ç ä¸º input_idsï¼ˆbatch=1ï¼‰"""
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    ids = tokenizer(text, return_tensors="pt", add_special_tokens=False).input_ids
    return ids

# ----------------------
# æ–‡ä»¶åå®‰å…¨åŒ–å·¥å…·
# ----------------------
_BAD_CHARS_RE = re.compile(r'[\\/:*?"<>|]+')

def _safe_stem(s: str, max_len: int = 100) -> str:
    s = s.strip()
    s = _BAD_CHARS_RE.sub("_", s)          # è·¯å¾„åˆ†éš”/éæ³•å­—ç¬¦ â†’ ä¸‹åˆ’çº¿
    s = re.sub(r"\s+", "_", s)             # è¿ç»­ç©ºç™½ â†’ ä¸‹åˆ’çº¿
    s = re.sub(r"[^A-Za-z0-9._\-]+", "_", s)  # å…¶å®ƒå¥‡æ€ªå­—ç¬¦ â†’ ä¸‹åˆ’çº¿
    s = s.strip("._-")
    if not s:
        s = "sample"
    if len(s) > max_len:
        s = s[:max_len]
    return s

def make_file_id(question: str, ex_id: str | None, index_k: int) -> str:
    """åŸºäº(ä¼˜å…ˆ)åŸå§‹idæˆ–é—®é¢˜æ–‡æœ¬ç”Ÿæˆç¨³å®šã€å®‰å…¨çš„æ–‡ä»¶åï¼ˆå«çŸ­hashé˜²å†²çªï¼‰"""
    base = ex_id if ex_id else question[:80]
    stem = _safe_stem(base, max_len=80)
    h = hashlib.sha1((ex_id or question).encode("utf-8")).hexdigest()[:8]
    return f"{index_k:05d}_{stem}_{h}"

@torch.inference_mode()
def step_decode_collect(model, tokenizer, input_ids: torch.Tensor, blocks, layers_idx: List[int],
                        max_new_tokens: int, temperature: float, top_p: float, stop_on_eos: bool) -> Dict[str, Any]:
    """
    å¢é‡è§£ç å¹¶åœ¨æ¯æ­¥æ”¶é›†æŒ‡å®šå±‚çš„éšçŠ¶æ€ã€‚
    è¿”å›ï¼š
      gen_ids: List[int]           # ç”Ÿæˆåºåˆ—ï¼ˆä¸å« promptï¼‰
      acts: np.ndarray[T, L, D]
      text: str                    # ç”Ÿæˆæ–‡æœ¬
    """
    device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    eos_id = tokenizer.eos_token_id

    # å®‰è£… hook
    recorder = ActivationRecorder(blocks, capture="output", dtype=torch.bfloat16)

    # å…ˆä¸€æ¬¡æ€§è·‘ promptï¼Œè·å–åˆå§‹ past
    out = model(input_ids=input_ids, use_cache=True)
    past_kv = out.past_key_values

    gen_ids: List[int] = []
    feats: List[torch.Tensor] = []

    # é€æ­¥ç”Ÿæˆ
    cur = input_ids[:, -1:]
    for step in range(max_new_tokens):
        recorder.start_step()

        # ä» forward è¾“å‡ºé‡Œæ‹¿ logits å’Œ past_kv
        out = model(input_ids=cur, use_cache=True, past_key_values=past_kv)
        logits = out.logits
        past_kv = out.past_key_values

        next_token_logits = logits[:, -1, :]  # (1, V)
        if temperature > 0.0:
            probs = torch.softmax(next_token_logits / max(1e-6, temperature), dim=-1)
            if top_p < 1.0:
                # ç®€åŒ–å¤„ç†ï¼šå¯¹ batch=1 ç›´æ¥åœ¨å…¨åˆ†å¸ƒä¸Šé‡‡æ ·å·²è¶³å¤Ÿç¨³å¦¥
                next_token = torch.multinomial(probs, num_samples=1)
            else:
                next_token = torch.multinomial(probs, num_samples=1)
        else:
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # è´ªå©ª

        # å–æœ¬æ­¥æ•è·åˆ°çš„ [L, D]
        step_feats = recorder.pop_step_features()  # [L, D], dtype=bfloat16(åœ¨CPU)
        feats.append(step_feats)
        tok = int(next_token.item())
        gen_ids.append(tok)

        if stop_on_eos and tok == eos_id:
            break

        cur = next_token

    recorder.remove()

    # å…³é”®ä¿®å¤ï¼šbfloat16 ä¸èƒ½ç›´æ¥ numpyï¼›å…ˆè½¬ float16ï¼Œå†è½¬ numpy
    acts = torch.stack(feats, dim=0).to(torch.float16).cpu().numpy()  # [T, L, D], np.float16
    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return {
        "gen_ids": gen_ids,
        "acts": acts,
        "text": gen_text,
    }

# def save_npz(sample_dir: str, file_stem: str, payload: Dict[str, Any]):
#     os.makedirs(sample_dir, exist_ok=True)
#     path = os.path.join(sample_dir, f"{file_stem}.npz")
#     np.savez_compressed(path, **payload)
#     return path

def save_npz(out_dir, file_stem, payload):
    os.makedirs(out_dir, exist_ok=True)
    tmp_path = tempfile.NamedTemporaryFile(suffix=".npz", delete=False)
    np.savez_compressed(tmp_path, **payload)
    tmp_path.close()
    final_path = os.path.join(out_dir, f"{file_stem}.npz")
    shutil.move(tmp_path.name, final_path)
    return final_path


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", type=str, default="configs/collect_acts.yaml", help="YAML é…ç½®è·¯å¾„")
    args = ap.parse_args()

    import yaml
    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    # 1) åŠ è½½æ¨¡å‹/åˆ†è¯å™¨
    mdl_cfg = QwenLoadConfig(
        model_id=cfg["model"]["model_id"],
        peft_model_id=cfg["model"].get("peft_model_id") or cfg["model"]["model_id"],
        attn_implementation=cfg["model"].get("attn_implementation", "flash_attention_2"),
        dtype=cfg["model"].get("dtype", "bfloat16"),
        device_map=cfg["model"].get("device_map", "auto"),
        local_files_only=cfg["model"].get("local_files_only", True),
        trust_remote_code=cfg["model"].get("trust_remote_code", True),
    )
    model, tokenizer = load_qwen(mdl_cfg)

    # 2) é€‰å±‚å¹¶è£…å¥½ blocks
    layers_idx: List[int] = cfg["collect"]["layers"]
    blocks, block_names = pick_blocks_by_indices(model, layers_idx)
    print(f"ğŸ“Œ é‡‡é›†å±‚: {block_names}")

    # 3) æ•°æ®
    limit = cfg["data"].get("limit")
    giter = iter_gsm8k(cfg["data"]["split"], limit=limit)

    # 4) è¾“å‡º
    out_dir = cfg["collect"]["save_dir"]
    os.makedirs(out_dir, exist_ok=True)
    write_txt = bool(cfg["io"].get("write_text_preview", True))

    # 5) è§£ç å‚æ•°
    max_new_tokens = int(cfg["collect"]["max_new_tokens"])
    temperature = float(cfg["collect"].get("temperature", 0.0))
    top_p = float(cfg["collect"].get("top_p", 1.0))
    stop_on_eos = bool(cfg["collect"].get("stop_on_eos", True))

    # 6) æç¤ºæ¨¡æ¿
    system = cfg["prompt"]["system"]
    user_suffix = cfg["prompt"].get("user_suffix", "")

    # 7) ä¸»å¾ªç¯
    meta_index = []
    for k, ex in enumerate(giter, start=1):
        raw_id = ex.get("id")
        qid = str(raw_id) if raw_id is not None else ""  # è¯­ä¹‰IDï¼ˆä¿ç•™åŸå§‹ï¼‰
        question = ex["question"]
        gold_norm = ex["gold_norm"]

        # ç”¨å®‰å…¨æ–‡ä»¶åï¼ˆä¸å«éæ³•å­—ç¬¦ï¼‰ï¼Œé˜²æ­¢ "1/6" ä¹‹ç±»çš„é—®é¢˜
        file_stem = make_file_id(question=question, ex_id=qid, index_k=k)

        messages = build_messages(question, system, user_suffix)
        input_ids = encode_chat(tokenizer, messages)

        with torch.no_grad():
            result = step_decode_collect(
                model, tokenizer, input_ids, blocks, layers_idx,
                max_new_tokens=max_new_tokens,
                temperature=temperature, top_p=top_p, stop_on_eos=stop_on_eos
            )

        gen_ids = result["gen_ids"]
        gen_text = result["text"]
        acts = result["acts"]  # [T, L, D]

        # ç”Ÿæˆçº§æŒ‡æ ‡
        pred_norm = extract_final_number(gen_text)
        success = equals_num(pred_norm, gold_norm)

        # æ¨ç†æ©ç ï¼ˆé’ˆå¯¹ç”Ÿæˆ tokensï¼‰
        reasoning_mask = compute_reasoning_mask(gen_ids, tokenizer, window=0)

        payload = {
            "question_id": qid if qid else question,       # è¯­ä¹‰IDä¿ç•™
            "gen_ids": np.array(gen_ids, dtype=np.int32),
            "acts": acts,                                  # [T, L, D] fp16
            "layers": np.array(layers_idx, dtype=np.int32),
            "reasoning_mask": np.array(reasoning_mask, dtype=bool),
            "gold": gold_norm if gold_norm is not None else "",
            "pred": pred_norm if pred_norm is not None else "",
            "success_flag": bool(success),
        }
        path = save_npz(out_dir, file_stem, payload)

        if write_txt:
            with open(os.path.join(out_dir, f"{file_stem}.txt"), "w", encoding="utf-8") as fw:
                fw.write(f"# id: {qid if qid else '(no-id)'}\n\n")
                fw.write("## Question\n")
                fw.write(question.strip() + "\n\n")
                fw.write("## Generated\n")
                fw.write(gen_text.strip() + "\n\n")
                fw.write(f"## Gold (norm): {gold_norm}\n")
                fw.write(f"## Pred (norm): {pred_norm}\n")
                fw.write(f"## Success: {success}\n")

        meta_index.append({
            "id": qid if qid else file_stem,
            "file": file_stem,
            "path": os.path.abspath(path),
            "T": int(acts.shape[0]),
            "L": int(acts.shape[1]),
            "D": int(acts.shape[2]),
            "success": bool(success),
        })

        if k % 10 == 0:
            print(f"âœ… å·²å®Œæˆ {k} æ¡ï¼Œæœ€åä¸€æ¡ä¿å­˜äº: {path}")

    # å†™ä¸€ä¸ªç®€å•ç´¢å¼•
    with open(os.path.join(out_dir, "_index.jsonl"), "w", encoding="utf-8") as fw:
        for row in meta_index:
            fw.write(json.dumps(row, ensure_ascii=False) + "\n")
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼Œå…± {len(meta_index)} æ¡ã€‚ç´¢å¼•: {os.path.join(out_dir, '_index.jsonl')}")


if __name__ == "__main__":
    main()
