# src/crb/pipelines/collect_activations.py
from __future__ import annotations
import os
import json
import argparse
import numpy as np
import torch
from typing import List, Dict, Any, Tuple

from transformers import PreTrainedTokenizer
from crb.models.qwen_loader import load_qwen, QwenLoadConfig
from crb.data.gsm8k import iter_gsm8k, extract_final_number, equals_num
from crb.instrumentation.layer_selectors import pick_blocks_by_indices
from crb.instrumentation.activation_recorder import ActivationRecorder
from crb.instrumentation.reasoning_mask import compute_reasoning_mask

def build_messages(question: str, system: str, user_suffix: str) -> List[Dict[str, str]]:
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": question.strip() + (user_suffix or "")},
    ]

def encode_chat(tokenizer: PreTrainedTokenizer, messages: List[Dict[str,str]]) -> torch.Tensor:
    """ä½¿ç”¨ Qwen/chat æ¨¡æ¿ç¼–ç ä¸º input_idsï¼ˆbatch=1ï¼‰"""
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    ids = tokenizer(text, return_tensors="pt", add_special_tokens=False).input_ids
    return ids

@torch.inference_mode()
def step_decode_collect(model, tokenizer, input_ids: torch.Tensor, blocks, layers_idx: List[int],
                        max_new_tokens: int, temperature: float, top_p: float, stop_on_eos: bool) -> Dict[str, Any]:
    """
    å¢žé‡è§£ç å¹¶åœ¨æ¯æ­¥æ”¶é›†æŒ‡å®šå±‚çš„éšçŠ¶æ€ã€‚
    è¿”å›žï¼š
      gen_ids: List[int]           # ç”Ÿæˆåºåˆ—ï¼ˆä¸å« promptï¼‰
      acts: np.ndarray[T, L, D]
      text: str                    # ç”Ÿæˆæ–‡æœ¬
    """
    device = next(model.parameters()).device
    input_ids = input_ids.to(device)
    eos_id = tokenizer.eos_token_id

    # å®‰è£… hook
    recorder = ActivationRecorder(blocks, capture="output", dtype=torch.bfloat16)

    # å…ˆä¸€æ¬¡æ€§è·‘ promptï¼ŒèŽ·å–åˆå§‹ past
    out = model(input_ids=input_ids, use_cache=True)
    past_kv = out.past_key_values

    gen_ids: List[int] = []
    feats: List[torch.Tensor] = []

    # é€æ­¥ç”Ÿæˆ
    cur = input_ids[:, -1:]
    for step in range(max_new_tokens):
        recorder.start_step()
        logits = model(input_ids=cur, use_cache=True, past_key_values=past_kv).logits
        past_kv = model.past_key_values  # æœ‰äº›å®žçŽ°æŠŠ past ä½œä¸ºå†…éƒ¨çŠ¶æ€æš´éœ²ï¼›ç¨³å¦¥èµ·è§ä¹Ÿä»Žè¾“å‡ºå–
        if past_kv is None and hasattr(out, "past_key_values"):
            past_kv = out.past_key_values

        next_token_logits = logits[:, -1, :]  # (1, V)
        if temperature > 0.0:
            probs = torch.softmax(next_token_logits / max(1e-6, temperature), dim=-1)
            if top_p < 1.0:
                # nucleus sampling
                sorted_probs, sorted_idx = torch.sort(probs, descending=True)
                cum = torch.cumsum(sorted_probs, dim=-1)
                mask = cum <= top_p
                mask[..., 0] = True
                idx_keep = sorted_idx[mask]
                probs_keep = probs.gather(-1, idx_keep.unsqueeze(0))
                probs_keep = probs_keep / probs_keep.sum(dim=-1, keepdim=True)
                next_token = idx_keep[torch.multinomial(probs_keep, num_samples=1)]
            else:
                next_token = torch.multinomial(probs, num_samples=1)
        else:
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # è´ªå©ª

        # å–æœ¬æ­¥æ•èŽ·åˆ°çš„ [L, D]
        step_feats = recorder.pop_step_features()  # [L, D]
        feats.append(step_feats)
        tok = int(next_token.item())
        gen_ids.append(tok)

        if stop_on_eos and tok == eos_id:
            break

        cur = next_token

    recorder.remove()

    acts = torch.stack(feats, dim=0).cpu().numpy().astype(np.float16)  # [T, L, D]
    gen_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return {
        "gen_ids": gen_ids,
        "acts": acts,
        "text": gen_text,
    }

def save_npz(sample_dir: str, qid: str, payload: Dict[str, Any]):
    os.makedirs(sample_dir, exist_ok=True)
    path = os.path.join(sample_dir, f"{qid}.npz")
    np.savez_compressed(path, **payload)
    return path

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", type=str, default="configs/collect_acts.yaml", help="YAML é…ç½®è·¯å¾„")
    args = ap.parse_args()

    import yaml
    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    # 1) åŠ è½½æ¨¡åž‹/åˆ†è¯å™¨
    mdl_cfg = QwenLoadConfig(
        model_id=cfg["model"]["model_id"],
        peft_model_id=cfg["model"].get("peft_model_id") or cfg["model"]["model_id"],
        attn_implementation=cfg["model"].get("attn_implementation", "flash_attention_2"),
        dtype=cfg["model"].get("dtype", "bfloat16"),
        device_map=cfg["model"].get("device_map", "auto"),
        local_files_only=cfg["model"].get("local_files_only", True),
        trust_remote_code=cfg["model"].get("trust_remote_code", True),
    )
    model, tokenizer = load_qwen(mdl_cfg)

    # 2) é€‰å±‚å¹¶è£…å¥½ blocks
    layers_idx: List[int] = cfg["collect"]["layers"]
    from crb.instrumentation.layer_selectors import pick_blocks_by_indices
    blocks, block_names = pick_blocks_by_indices(model, layers_idx)
    print(f"ðŸ“Œ é‡‡é›†å±‚: {block_names}")

    # 3) æ•°æ®
    limit = cfg["data"].get("limit")
    giter = iter_gsm8k(cfg["data"]["split"], limit=limit)

    # 4) è¾“å‡º
    out_dir = cfg["collect"]["save_dir"]
    os.makedirs(out_dir, exist_ok=True)
    write_txt = bool(cfg["io"].get("write_text_preview", True))

    # 5) è§£ç å‚æ•°
    max_new_tokens = int(cfg["collect"]["max_new_tokens"])
    temperature = float(cfg["collect"].get("temperature", 0.0))
    top_p = float(cfg["collect"].get("top_p", 1.0))
    stop_on_eos = bool(cfg["collect"].get("stop_on_eos", True))

    # 6) æç¤ºæ¨¡æ¿
    system = cfg["prompt"]["system"]
    user_suffix = cfg["prompt"].get("user_suffix", "")

    # 7) ä¸»å¾ªçŽ¯
    meta_index = []
    for k, ex in enumerate(giter, start=1):
        qid = str(ex["id"])
        question = ex["question"]
        gold_norm = ex["gold_norm"]

        messages = build_messages(question, system, user_suffix)
        input_ids = encode_chat(tokenizer, messages)

        with torch.no_grad():
            result = step_decode_collect(
                model, tokenizer, input_ids, blocks, layers_idx,
                max_new_tokens=max_new_tokens,
                temperature=temperature, top_p=top_p, stop_on_eos=stop_on_eos
            )

        gen_ids = result["gen_ids"]
        gen_text = result["text"]
        acts = result["acts"]  # [T, L, D]

        # ç”Ÿæˆçº§æŒ‡æ ‡
        pred_norm = extract_final_number(gen_text)
        success = equals_num(pred_norm, gold_norm)

        # æŽ¨ç†æŽ©ç ï¼ˆé’ˆå¯¹ç”Ÿæˆ tokensï¼‰
        reasoning_mask = compute_reasoning_mask(gen_ids, tokenizer, window=0)

        payload = {
            "question_id": qid,
            "gen_ids": np.array(gen_ids, dtype=np.int32),
            "acts": acts,                                  # [T, L, D] fp16
            "layers": np.array(layers_idx, dtype=np.int32),
            "reasoning_mask": np.array(reasoning_mask, dtype=bool),
            "gold": gold_norm if gold_norm is not None else "",
            "pred": pred_norm if pred_norm is not None else "",
            "success_flag": bool(success),
        }
        path = save_npz(out_dir, qid, payload)

        if write_txt:
            with open(os.path.join(out_dir, f"{qid}.txt"), "w", encoding="utf-8") as fw:
                fw.write(f"# id: {qid}\n\n")
                fw.write("## Question\n")
                fw.write(question.strip() + "\n\n")
                fw.write("## Generated\n")
                fw.write(gen_text.strip() + "\n\n")
                fw.write(f"## Gold (norm): {gold_norm}\n")
                fw.write(f"## Pred (norm): {pred_norm}\n")
                fw.write(f"## Success: {success}\n")

        meta_index.append({
            "id": qid,
            "path": os.path.abspath(path),
            "T": int(acts.shape[0]),
            "L": int(acts.shape[1]),
            "D": int(acts.shape[2]),
            "success": bool(success),
        })

        if k % 10 == 0:
            print(f"âœ… å·²å®Œæˆ {k} æ¡ï¼Œæœ€åŽä¸€æ¡ä¿å­˜äºŽ: {path}")

    # å†™ä¸€ä¸ªç®€å•ç´¢å¼•
    with open(os.path.join(out_dir, "_index.jsonl"), "w", encoding="utf-8") as fw:
        for row in meta_index:
            fw.write(json.dumps(row, ensure_ascii=False) + "\n")
    print(f"ðŸŽ‰ å…¨éƒ¨å®Œæˆï¼Œå…± {len(meta_index)} æ¡ã€‚ç´¢å¼•: {os.path.join(out_dir, '_index.jsonl')}")


if __name__ == "__main__":
    main()
